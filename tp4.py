# -*- coding: utf-8 -*-
"""TP4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KL_mKHub5W96grawR91nibfRigHGRB05

EXE 1
"""

import gymnasium as gym
import numpy as np

# Initialisation de l'environnement
env = gym.make("Taxi-v3")

# Nombre d'états et d'actions
state_size = env.observation_space.n
action_size = env.action_space.n

# Table de politique (probabilités égales pour chaque action)
policy_table = np.full((state_size, action_size), 1/action_size)

# Table de valeurs initialisée à zéro
value_table = np.zeros(state_size)

# Affichage des premières lignes
print("Policy table (extrait):\n", policy_table[:5])
print("\nValue table (extrait):\n", value_table[:5])

# Simulation d'un agent aléatoire sur 20 épisodes
for episode in range(20):
    state, _ = env.reset()
    done = False
    total_reward = 0

    print(f"\n=== Épisode {episode + 1} ===")
    while not done:
        action = np.random.choice(action_size, p=policy_table[state])
        next_state, reward, done, _, _ = env.step(action)

        print(f"État: {state}, Action: {action}, Récompense: {reward}")
        total_reward += reward
        state = next_state

    print(f"Récompense totale de l'épisode: {total_reward}")

# Hyperparamètres
gamma = 0.99          # Facteur de discount
lr_policy = 0.1       # Taux d'apprentissage pour la politique
clip_epsilon = 0.2    # Paramètre de clipping pour PPO

def update_ppo(episode_states, episode_actions, episode_rewards):
    # Calcul des récompenses cumulées (discounted rewards)
    discounted_rewards = []
    R = 0
    for r in reversed(episode_rewards):
        R = r + gamma * R
        discounted_rewards.insert(0, R)

    # Normalisation des récompenses
    discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-8)

    # Mise à jour de la politique et de la fonction de valeur
    for state, action, G in zip(episode_states, episode_actions, discounted_rewards):
        # Calcul de l'avantage
        advantage = G - value_table[state]

        # Mise à jour de la politique avec clipping
        old_prob = policy_table[state][action]
        new_prob = old_prob * np.exp(lr_policy * advantage)

        # Clipping pour éviter des mises à jour trop grandes
        ratio = new_prob / old_prob
        clipped_ratio = np.clip(ratio, 1 - clip_epsilon, 1 + clip_epsilon)
        policy_table[state][action] = min(ratio * advantage, clipped_ratio * advantage)

        # Normalisation des probabilités
        policy_table[state] /= np.sum(policy_table[state])

        # Mise à jour de la fonction de valeur
        value_table[state] += lr_policy * advantage

# Exemple d'utilisation après un épisode
episode_states = [0, 1, 2]  # Exemple fictif
episode_actions = [1, 2, 0] # Exemple fictif
episode_rewards = [1, -1, 2] # Exemple fictif
update_ppo(episode_states, episode_actions, episode_rewards)

def evaluate_agent(num_episodes=20):
    total_rewards = []
    for _ in range(num_episodes):
        state, _ = env.reset()
        done = False
        episode_reward = 0

        while not done:
            action = np.argmax(policy_table[state])  # Choix de l'action optimale
            state, reward, done, _, _ = env.step(action)
            episode_reward += reward

        total_rewards.append(episode_reward)

    print(f"Récompenses moyennes sur {num_episodes} épisodes: {np.mean(total_rewards)}")
    return total_rewards

# Entraînement sur plusieurs épisodes (exemple simplifié)
for episode in range(100):  # 100 épisodes d'entraînement
    states, actions, rewards = [], [], []
    state, _ = env.reset()
    done = False

    while not done:
        action = np.random.choice(action_size, p=policy_table[state])
        next_state, reward, done, _, _ = env.step(action)

        states.append(state)
        actions.append(action)
        rewards.append(reward)
        state = next_state

    update_ppo(states, actions, rewards)

# Évaluation finale
print("\n=== Performances après entraînement ===")
evaluate_agent()