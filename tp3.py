# -*- coding: utf-8 -*-
"""TP2-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GiyMWbz0P8vMcHgCHR3ynoWj4XO6Lf74
"""

# import gymnasium as gym
# import numpy as np
# #charger l'enverenement
# env = gym.make("FrozenLake-v1",is_slippery = true )
import gymnasium as gym
import numpy as np

# Charger l'environnement FrozenLake-v1
env = gym.make("FrozenLake-v1", is_slippery=True, render_mode="human")

# Afficher les informations de l'espace d'états et d'actions
num_states = env.observation_space.n
num_actions = env.action_space.n
print("Espace d'états:", env.observation_space)
print("Espace d'actions:", env.action_space)
#----------exe2
# Création et initialisation de la Q-Table
q_table = np.zeros((num_states, num_actions))

# Affichage de la Q-Table avant l'apprentissage
print("Q-Table initiale:")
print(q_table)

# Vérification que chaque état a une liste de valeurs associées aux actions possibles
for state in range(num_states):
    print(f"État {state}: {q_table[state]}")
#------------

# Définir le nombre d'épisodes et de pas par épisode
num_episodes = 5
max_steps = 100

for episode in range(num_episodes):
    observation, info = env.reset()
    print(f"\nÉpisode {episode + 1} :")

    for step in range(max_steps):
        action = env.action_space.sample()  # Prendre une action aléatoire
        next_observation, reward, terminated, truncated, info = env.step(action)

        print(f"Étape {step + 1}: Action={action}, Observation={next_observation}, Récompense={reward}")

        if terminated or truncated:
            print("Épisode terminé.\n")
            break

# Fermer l'environnement
env.close()

#EXE3
import gymnasium as gym
import numpy as np

# Charger l'environnement FrozenLake-v1
env = gym.make("FrozenLake-v1", is_slippery=True, render_mode="human")

# Afficher les informations de l'espace d'états et d'actions
num_states = env.observation_space.n
num_actions = env.action_space.n
print("Espace d'états:", num_states)
print("Espace d'actions:", num_actions)

# Création et initialisation de la Q-Table
q_table = np.zeros((num_states, num_actions))

# Définition des hyperparamètres
alpha = 0.1  # Taux d'apprentissage
gamma = 0.99  # Facteur de discount
epsilon = 1.0  # Probabilité d'exploration
epsilon_decay = 0.995
epsilon_min = 0.01
num_episodes = 5000
max_steps = 100

# Boucle d'apprentissage du Q-Learning
for episode in range(num_episodes):
    state, info = env.reset()

    for step in range(max_steps):
        # Choisir une action (exploration ou exploitation)
        if np.random.rand() < epsilon:
            action = env.action_space.sample()  # Action aléatoire (exploration)
        else:
            action = np.argmax(q_table[state])  # Action optimale connue (exploitation)

        # Exécuter l'action
        next_state, reward, terminated, truncated, info = env.step(action)

        # Mettre à jour la Q-Table
        best_next_action = np.argmax(q_table[next_state])
        q_table[state, action] += alpha * (reward + gamma * q_table[next_state, best_next_action] - q_table[state, action])

        # Passer à l'état suivant
        state = next_state

        if terminated or truncated:
            break

    # Réduire epsilon pour diminuer l'exploration au fil du temps
    epsilon = max(epsilon_min, epsilon * epsilon_decay)

# Affichage de la Q-Table après l'apprentissage
print("Q-Table après apprentissage:")
print(q_table)

# Fermer l'environnement
env.close()