# Machine Learning II - Reinforcement Learning

**UniversitÃ© Mohamed Premier Oujda**  
**Ã‰cole Nationale de l'Intelligence Artificielle et du Digital Berkane**  
**AnnÃ©e universitaire : 2024 / 2025**  
**Professeur : Mohamed Khalifa BOUTAHIR**

---

## ğŸ“š Table des MatiÃ¨res
1. [TP1 - DÃ©couverte d'OpenAI Gym](#-tp1---dÃ©couverte-dopenai-gym)
2. [TP2 - Q-Learning avec FrozenLake](#-tp2---q-learning-avec-frozenlake)
3. [TP3 - Optimisation des Feux de Circulation](#-tp3---optimisation-des-feux-de-circulation)
4. [TP4 - PPO avec Taxi-v3](#-tp4---ppo-avec-taxi-v3)
5. [Structure du Repository](#-structure-du-repository)
6. [Installation](#-installation)
7. [Ressources](#-ressources)

---

## ğŸ¯ TP1 - DÃ©couverte d'OpenAI Gym

### Objectifs
- Se familiariser avec les environnements de Reinforcement Learning
- Explorer l'environnement CartPole-v1
- ImplÃ©menter des politiques alÃ©atoires

### RÃ©sultats ClÃ©s
- ComprÃ©hension des espaces d'Ã©tats et d'actions
- Performance des actions alÃ©atoires : ~20 pas avant Ã©chec
- Visualisation des Ã©tats et rÃ©compenses

### Fichiers Principaux
```bash
TP1/
â”œâ”€â”€ exercice1_env_exploration.py
â”œâ”€â”€ exercice2_rewards_analysis.py
â””â”€â”€ exercice3_manual_control.py
```
---

## â„ï¸ TP2 - Q-Learning avec FrozenLake 

### Objectifs
-ImplÃ©menter l'algorithme Q-Learning

-Comprendre l'exploration vs exploitation

-Analyser la convergence de la Q-table

### Algorithmes ClÃ©s
  ```bash
  Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
  ```

### RÃ©sultats

| MÃ©trique                        |Valeur|
----------------------------------|-------|
|Taux de rÃ©ussite (alÃ©atoire)      | 1.5% |
|Taux de rÃ©ussite (aprÃ¨s Q-Learning) |75% |
|Ã‰pisodes d'entraÃ®nement             | 5000 |

---

## ğŸš¦ TP3 - Optimisation des Feux de Circulation

### Comparaison Q-Learning vs SARSA

  ```bash
  # Q-Learning (off-policy)
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]

# SARSA (on-policy) 
Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
  ```

### Performances

|Algorithme	   |RÃ©duction Temps d'Attente|
---------------|-------------------------|
|Q-Learning	   |82%|
|SARSA	       |78%|

---

## ğŸš– TP4 - PPO avec Taxi-v3

### Proximal Policy Optimization

### Fonction objectif avec clipping :

  ```bash
  L(Î¸) = ğ”¼[min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)]
  ```

#### RÃ©sultats

| Phase               | Taux de RÃ©ussite | Steps Moyens |
|---------------------|------------------|--------------|
| Avant entraÃ®nement  | 0%               | 200+         |
| AprÃ¨s 1000 Ã©pisodes | 92%              | 15.2         |


### Architecture

-Table de politique : (500 Ã©tats Ã— 6 actions)

-Table de valeurs : (500 Ã©tats)

### ğŸ“‚ Structure du Repository
  ```bash
MLII_RL_TP/
â”œâ”€â”€ TP1/                  # DÃ©couverte OpenAI Gym
â”œâ”€â”€ TP2/                  # Q-Learning FrozenLake
â”œâ”€â”€ TP3/                  # Feux de Circulation
â”œâ”€â”€ TP4/                  # PPO Taxi-v3
â”œâ”€â”€ requirements.txt       # DÃ©pendances
â””â”€â”€ README.md             # Ce fichier
  ```

### ğŸ› ï¸ Installation
1. Cloner le repository :
  ```bash
  git clone https://github.com/votre-utilisateur/ML2.git
  ```
2. Installer les dÃ©pendances :
  ```bash
  pip install -r requirements.txt
  ```
3. ExÃ©cuter un TP :
  ```bash
  python TP1/exercice1.py
  ```
---

## ğŸ“š Ressources

### 1.Documentation :

-[OpenAI Gymnasium] (https://gymnasium.farama.org/)

-[Stable Baselines3] (https://stable-baselines3.readthedocs.io/en/master/)

### 2.Livres :

-* "Reinforcement Learning: An Introduction" - Sutton & Barto

-* "Deep Reinforcement Learning Hands-On" - Maxim Lapan

### Articles :

* Proximal Policy Optimization (PPO) - Schulman et al. 2017

* Q-Learning Convergence - Watkins & Dayan 1992

### ğŸ“ Remarques Finales
Ce repository couvre les fondamentaux du Reinforcement Learning :

*Des algorithmes tabulaires (Q-Learning, SARSA)

*Aux mÃ©thodes de politique avancÃ©es (PPO)

*Avec applications sur des problÃ¨mes concrets

*Les visualisations et comparaisons permettent de bien comprendre les forces/faiblesses de chaque approche.
