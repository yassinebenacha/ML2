# -*- coding: utf-8 -*-
"""TP1 ML2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/197uHz6vmL3OktyWhK8j60illTS-ON3rB
"""

import numpy as np
import random

# D√©finition de l'environnement (grille 5x5)
GRID_SIZE = 5
START = (0, 0)
TREASURE = (3, 2)
TRAPS = [(1, 1), (2, 3)]
#D√©finition des actions possibles
ACTIONS = ["UP", "DOWN", "LEFT", "RIGHT"]
ACTION_MAP = {
    "UP": (-1, 0),
    "DOWN": (1, 0),
    "LEFT": (0, -1),
    "RIGHT": (0, 1)
}

# Initialisation de la table Q (Q-table)
q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))

# Param√®tres d'apprentissage
alpha = 0.1  # Taux d'apprentissage
gamma = 0.9  # Facteur de r√©compense future
epsilon = 1.0  # Exploration vs exploitation
epsilon_decay = 0.99
min_epsilon = 0.1
episodes = 5000

# Fonction pour obtenir la r√©compense
def get_reward(state):
    if state == TREASURE:
        return 10  # R√©compense pour atteindre le tr√©sor
    if state in TRAPS:
        return -10  # P√©nalit√© pour tomber dans un pi√®ge
    return -1  # Co√ªt pour chaque d√©placement

# Fonction pour effectuer une action et obtenir le nouvel √©tat
def take_action(state, action):
    new_state = (state[0] + ACTION_MAP[action][0], state[1] + ACTION_MAP[action][1])
    # V√©rifier si l'agent sort des limites
    if new_state[0] < 0 or new_state[0] >= GRID_SIZE or new_state[1] < 0 or new_state[1] >= GRID_SIZE:
        return state  # Rester sur place si mouvement invalide
    return new_state

# Entra√Ænement avec Q-Learning
for episode in range(episodes):
    state = START
    done = False
    while not done:
        # Choisir une action (exploration vs exploitation)
        if random.uniform(0, 1) < epsilon:
            action_index = random.randint(0, len(ACTIONS) - 1)  # Exploration
        else:
            action_index = np.argmax(q_table[state[0], state[1]])  # Exploitation

        action = ACTIONS[action_index]
        new_state = take_action(state, action)
        reward = get_reward(new_state)

        # Mettre √† jour la Q-table
        q_table[state[0], state[1], action_index] = (1 - alpha) * q_table[state[0], state[1], action_index] + \
            alpha * (reward + gamma * np.max(q_table[new_state[0], new_state[1]]))

        state = new_state
        if state == TREASURE or state in TRAPS:
            done = True  # Fin de l'√©pisode

    # Diminuer epsilon (moins d'exploration au fil du temps)
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

print("Entra√Ænement termin√© ! üéâ")

# Affichage de la strat√©gie optimale apprise
policy = np.array([[" " for _ in range(GRID_SIZE)] for _ in range(GRID_SIZE)])
for i in range(GRID_SIZE):
    for j in range(GRID_SIZE):
        if (i, j) == TREASURE:
            policy[i][j] = "üèÜ"
        elif (i, j) in TRAPS:
            policy[i][j] = "üíÄ"
        else:
            best_action = np.argmax(q_table[i, j])
            policy[i][j] = ACTIONS[best_action][0]  # Premi√®re lettre de l'action

print("\nStrat√©gie optimale apprise :")
for row in policy:
    print(" ".join(row))